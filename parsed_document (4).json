[
  {
    "subject_title": "LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS",
    "section_type": "Document Title",
    "starting_page_no": 1,
    "ending_page_no": 1,
    "entities": [
      {
        "company name": "Microsoft Corporation"
      },
      {
        "authors": [
          "Edward Hu",
          "Yelong Shen",
          "Phillip Wallis",
          "Zeyuan Allen-Zhu",
          "Yuanzhi Li",
          "Shean Wang",
          "Lu Wang",
          "Weizhu Chen"
        ]
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "ABSTRACT",
    "section_type": "Abstract",
    "starting_page_no": 1,
    "ending_page_no": 1,
    "entities": [
      {
        "model name": "GPT-3"
      },
      {
        "parameter count": "175B"
      },
      {
        "company name": "Microsoft"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "INTRODUCTION",
    "section_type": "Section",
    "starting_page_no": 1,
    "ending_page_no": 1,
    "entities": [
      {
        "model names": [
          "RoBERTa",
          "DeBERTa",
          "GPT-2",
          "GPT-3"
        ]
      },
      {
        "authors": [
          "Radford et al.",
          "Liu et al.",
          "Brown et al."
        ]
      },
      {
        "publication years": [
          "b",
          "2019",
          "2020"
        ]
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Introduction",
    "section_type": "Abstract",
    "starting_page_no": 2,
    "ending_page_no": 2,
    "entities": [
      {
        "author": "Houlsby et al."
      },
      {
        "author": "Rebuf\ufb01 et al."
      },
      {
        "author": "Li et al."
      },
      {
        "author": "Aghajanyan et al."
      },
      {
        "model": "GPT-3"
      },
      {
        "publication year": "2019"
      },
      {
        "publication year": "2017"
      },
      {
        "publication year": "2021"
      },
      {
        "publication year": "2020"
      },
      {
        "publication year": "2018"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Terminologies and Conventions",
    "section_type": "Definition",
    "starting_page_no": 2,
    "ending_page_no": 2,
    "entities": [
      {
        "model": "Transformer"
      },
      {
        "author": "Vaswani et al."
      },
      {
        "author": "Brown et al."
      },
      {
        "author": "Loshchilov & Hutter"
      },
      {
        "author": "Kingma & Ba"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "PROBLEM STATEMENT",
    "section_type": "Problem Definition",
    "starting_page_no": 2,
    "ending_page_no": 2,
    "entities": [
      {
        "model": "GPT"
      },
      {
        "author": "Radford et al."
      },
      {
        "author": "Brown et al."
      },
      {
        "author": "Vaswani et al."
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Full Fine-Tuning",
    "section_type": "Introduction",
    "starting_page_no": 3,
    "ending_page_no": 3,
    "entities": [
      {
        "model": "GPT-3",
        "parameter count": "175Billion"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Are Existing Solutions Good Enough?",
    "section_type": "Related Work",
    "starting_page_no": 3,
    "ending_page_no": 3,
    "entities": [
      {
        "model": "GPT-2",
        "authors": [
          "Houlsby et al.",
          "Rebuf\ufb01 et al.",
          "Pfeiffer et al.",
          "R \u00a8uckl\u00b4e et al.",
          "Li & Liang",
          "Lester et al.",
          "Hambardzumyan et al.",
          "Liu et al.",
          "Lin et al.",
          "Ba et al.",
          "Radford et al.",
          "Shoeybi et al.",
          "Lepikhin et al."
        ]
      }
    ],
    "subsections": []
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 4,
    "ending_page_no": 4,
    "entities": [],
    "subsections": [],
    "raw_text": "Batch Size 32 16 1\nSequence Length 512 256 128\nj\u0002j 0.5M 11M 11M\nFine-Tune/LoRA 1449.4\u00060.8 338.0 \u00060.6 19.8 \u00062.7\nAdapterL1482.0\u00061.0 (+2.2%) 354.8 \u00060.5 (+5.0%) 23.9 \u00062.1 (+20.7%)\nAdapterH1492.2\u00061.0 (+3.0%) 366.3 \u00060.5 (+8.4%) 25.8 \u00062.2 (+30.3%)\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. \u201c j\u0002j\u201d denotes the number of trainable\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signi\ufb01cant in an\nonline, short-sequence-length scenario. See the full study in Appendix B.\n4 O URMETHOD\nWe describe the simple design of LoRA and its practical bene\ufb01ts. The principles outlined here apply\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\nlanguage models in our experiments as the motivating use case.\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\nA neural network contains many dense layers which perform matrix multiplication. The weight\nmatrices in these layers typically have full-rank. When adapting to a speci\ufb01c task, Aghajanyan et al.\n(2020) shows that the pre-trained language models have a low \u201cinstrisic dimension\u201d and can still\nlearn ef\ufb01ciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\nsize the updates to the weights also have a low \u201cintrinsic rank\u201d during adaptation. For a pre-trained\nweight matrix W02Rd\u0002k, we constrain its update by representing the latter with a low-rank de-\ncomposition W0+ \u0001W=W0+BA, whereB2Rd\u0002r;A2Rr\u0002k, and the rank r\u001cmin(d;k).\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\nparameters. Note both W0and\u0001W=BAare multiplied with the same input, and their respective\noutput vectors are summed coordinate-wise. For h=W0x, our modi\ufb01ed forward pass yields:\nh=W0x+ \u0001Wx=W0x+BAx (3)\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\nzero forB, so\u0001W=BAis zero at the beginning of training. We then scale \u0001Wx by\u000b\nr, where\u000b\nis a constant in r. When optimizing with Adam, tuning \u000bis roughly the same as tuning the learning\nrate if we scale the initialization appropriately. As a result, we simply set \u000bto the \ufb01rstrwe try\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\nr(Yang & Hu, 2021).\nA Generalization of Full Fine-tuning. A more general form of \ufb01ne-tuning allows the training of\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\nness of full \ufb01ne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\nto training the original model, while adapter-based methods converges to an MLP and pre\ufb01x-based\nmethods to a model that cannot take long input sequences.\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\u0002k.\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\nthen adding a different B0A0, a quick operation with very little memory overhead. Critically, this\n2They represent a negligible number of parameters compared to weights.\n3An inevitability when adapting to hard tasks.\n4"
  },
  {
    "subject_title": "APPLYING LORA TO TRANSFORMER",
    "section_type": "Methodology",
    "starting_page_no": 5,
    "ending_page_no": 5,
    "entities": [
      {
        "model": "Transformer"
      },
      {
        "module": "self-attention"
      },
      {
        "module": "MLP"
      },
      {
        "parameter": "Wq"
      },
      {
        "parameter": "Wk"
      },
      {
        "parameter": "Wv"
      },
      {
        "parameter": "Wo"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "PRACTICAL BENEFITS AND LIMITATIONS",
    "section_type": "Advantages and Limitations",
    "starting_page_no": 5,
    "ending_page_no": 5,
    "entities": [
      {
        "model": "GPT-3 175B"
      },
      {
        "metric": "VRAM usage"
      },
      {
        "metric": "checkpoint size"
      },
      {
        "metric": "speedup"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "EMPIRICAL EXPERIMENTS",
    "section_type": "Experiments",
    "starting_page_no": 5,
    "ending_page_no": 5,
    "entities": [
      {
        "model": "RoBERTa"
      },
      {
        "model": "DeBERTa"
      },
      {
        "model": "GPT-2"
      },
      {
        "model": "GPT-3 175B"
      },
      {
        "dataset": "GLUE"
      },
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "SAMSum"
      }
    ],
    "subsections": [
      {
        "subject_title": "BASELINES",
        "section_type": "Sub-experiment",
        "starting_page_no": 5,
        "ending_page_no": 5,
        "entities": [
          {
            "baseline": "Fine-Tuning (FT)"
          },
          {
            "baseline": "FTTop2"
          }
        ],
        "subsections": []
      }
    ]
  },
  {
    "subject_title": "Model & Method",
    "section_type": "Table",
    "starting_page_no": 6,
    "ending_page_no": 6,
    "entities": [
      {
        "model": "RoBERTa base"
      },
      {
        "model": "RoBERTa large"
      },
      {
        "model": "DeBERTa XXL"
      },
      {
        "parameter": "125.0M"
      },
      {
        "parameter": "0.1M"
      },
      {
        "parameter": "0.3M"
      },
      {
        "parameter": "0.9M"
      },
      {
        "parameter": "355.0M"
      },
      {
        "parameter": "0.8M"
      },
      {
        "parameter": "3.0M"
      },
      {
        "parameter": "6.0M"
      },
      {
        "parameter": "1500.0M"
      },
      {
        "parameter": "4.7M"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Description of Adaptation Methods",
    "section_type": "Text",
    "starting_page_no": 6,
    "ending_page_no": 6,
    "entities": [
      {
        "author": "Houlsby et al."
      },
      {
        "author": "Zaken et al."
      },
      {
        "author": "Li & Liang"
      },
      {
        "author": "Lin et al."
      },
      {
        "author": "Pfeiffer et al."
      },
      {
        "author": "R\u00fcckl\u00b4e et al."
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Model & Method",
    "section_type": "Experiment Results",
    "starting_page_no": 7,
    "ending_page_no": 7,
    "entities": [
      {
        "model": "GPT-2 M"
      },
      {
        "model": "GPT-2 L"
      },
      {
        "metric": "BLEU"
      },
      {
        "metric": "NIST"
      },
      {
        "metric": "MET"
      },
      {
        "metric": "ROUGE-L"
      },
      {
        "metric": "CIDEr"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "5.2 ROBERTA BASE/LARGE",
    "section_type": "Model Description",
    "starting_page_no": 7,
    "ending_page_no": 7,
    "entities": [
      {
        "model": "RoBERTa"
      },
      {
        "author": "Liu et al."
      },
      {
        "year": "2019"
      },
      {
        "model": "BERT"
      },
      {
        "author": "Devlin et al."
      },
      {
        "year": "2019"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "5.3 DeBERTa AXXL",
    "section_type": "Model Description",
    "starting_page_no": 7,
    "ending_page_no": 7,
    "entities": [
      {
        "model": "DeBERTa"
      },
      {
        "author": "He et al."
      },
      {
        "year": "2021"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "5.4 GPT-2 MEDIUM/LARGE",
    "section_type": "Experiment Description",
    "starting_page_no": 7,
    "ending_page_no": 7,
    "entities": [
      {
        "model": "GPT-2 medium"
      },
      {
        "model": "GPT-2 large"
      },
      {
        "author": "Radford et al."
      },
      {
        "year": "b"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Model & Method",
    "section_type": "Table",
    "starting_page_no": 8,
    "ending_page_no": 8,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "MultiNLI-matched"
      },
      {
        "dataset": "SAMSum"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Scaling up to GPT-3 175B",
    "section_type": "Text",
    "starting_page_no": 8,
    "ending_page_no": 8,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "parameter count": "175 billion"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Figure 2: GPT-3 175B validation accuracy",
    "section_type": "Figure",
    "starting_page_no": 8,
    "ending_page_no": 8,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "MultiNLI-matched"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Related Works",
    "section_type": "Section",
    "starting_page_no": 8,
    "ending_page_no": 8,
    "entities": [
      {
        "model": "Transformer"
      },
      {
        "paper": "Vaswani et al. (2017)"
      },
      {
        "paper": "Radford et al. (a)"
      },
      {
        "paper": "Devlin et al. (2019b)"
      },
      {
        "paper": "Radford et al. (b)"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Transformer Language Models",
    "section_type": "Introduction",
    "starting_page_no": 9,
    "ending_page_no": 9,
    "entities": [
      {
        "company name": "None"
      },
      {
        "publication year": "2020"
      },
      {
        "model name": "GPT-3"
      },
      {
        "parameter count": "175B"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Prompt Engineering and Fine-Tuning",
    "section_type": "Subsection",
    "starting_page_no": 9,
    "ending_page_no": 9,
    "entities": [
      {
        "model name": "GPT-3 175B"
      },
      {
        "publication year": "2020"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Parameter-Efficient Adaptation",
    "section_type": "Subsection",
    "starting_page_no": 9,
    "ending_page_no": 9,
    "entities": [
      {
        "model name": "COMPACTER"
      },
      {
        "publication year": "2021"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Low-Rank Structures in Deep Learning",
    "section_type": "Subsection",
    "starting_page_no": 9,
    "ending_page_no": 9,
    "entities": [
      {
        "publication year": "2016"
      },
      {
        "publication year": "2010"
      },
      {
        "publication year": "2018"
      },
      {
        "publication year": "2013"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "UNDERSTANDING THE LOW-RANK UPDATES",
    "section_type": "Section",
    "starting_page_no": 9,
    "ending_page_no": 9,
    "entities": [
      {
        "model name": "LoRA"
      },
      {
        "model name": "GPT-3 175B"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?",
    "section_type": "Section",
    "starting_page_no": 10,
    "ending_page_no": 10,
    "entities": [
      {
        "model": "GPT-3 175B"
      },
      {
        "parameter budget": "18M"
      },
      {
        "storage size": "35MB"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "7.2 WHAT IS THE OPTIMAL RANKrFOR LORA?",
    "section_type": "Section",
    "starting_page_no": 10,
    "ending_page_no": 10,
    "entities": [
      {
        "model": "GPT-2"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 11,
    "ending_page_no": 11,
    "entities": [],
    "subsections": [],
    "raw_text": "Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\u0014i\u00148) is\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1\u0014j\u001464)? We mea-\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\npendix G for a more formal discussion)\n\u001e(Ar=8;Ar=64;i;j) =jjUi>\nAr=8Uj\nAr=64jj2\nF\nmin(i;j)2[0;1] (4)\nwhereUi\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\n\u001e(\u0001)has a range of [0;1], where 1represents a complete overlap of subspaces and 0a complete\nseparation. See Figure 3 for how \u001echanges as we vary iandj. We only look at the 48th layer\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\nin Section H.1.\n0.00.20.40.60.81.0\n1\n6\n12\n18\n23\n29\n35\n40\n46\n52\n58\nj12345678iWq\n1\n6\n12\n18\n23\n29\n35\n40\n46\n52\n58\njWv\n12345678\njWq\n12345678\njWv\n(Ar=64,Ar=8,i,j)\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both \u0001Wqand\u0001Wv.\nThe third and the fourth \ufb01gures zoom in on the lower-left triangle in the \ufb01rst two \ufb01gures. The top\ndirections in r= 8are included in r= 64 , and vice versa.\nWe make an important observation from Figure 3.\nDirections corresponding to the top singular vector overlap signi\ufb01cantly between\nAr=8andAr=64, while others do not. Speci\ufb01cally, \u0001Wv(resp. \u0001Wq) ofAr=8\nand\u0001Wv(resp. \u0001Wq) ofAr=64share a subspace of dimension 1 with normalized\nsimilarity>0:5, providing an explanation of why r= 1 performs quite well in our\ndownstream tasks for GPT-3.\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\ncan indeed have a very low rank.\nSubspace similarity between different random seeds. We further con\ufb01rm this by plotting the\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\n\u0001Wqappears to have a higher \u201cintrinsic rank\u201d than \u0001Wv, since more common singular value direc-\ntions are learned by both runs for \u0001Wq, which is in line with our empirical observation in Table 6.\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\nsingular value directions with each other.\n7.3 H OWDOES THE ADAPTATION MATRIX \u0001WCOMPARE TO W?\nWe further investigate the relationship between \u0001WandW. In particular, does \u0001Whighly correlate\nwithW? (Or mathematically, is \u0001Wmostly contained in the top singular directions of W?) Also,\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices \u2013 we stick with\nAfor our experiments.\n11"
  },
  {
    "subject_title": "Figure 4",
    "section_type": "Figure",
    "starting_page_no": 12,
    "ending_page_no": 12,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "layer": "48-th layer"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Table 7",
    "section_type": "Table",
    "starting_page_no": 12,
    "ending_page_no": 12,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "layer": "48th layer"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "CONCLUSION AND FUTURE WORK",
    "section_type": "Conclusion",
    "starting_page_no": 12,
    "ending_page_no": 12,
    "entities": [
      {
        "model": "Transformer language models"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 13,
    "ending_page_no": 13,
    "entities": [],
    "subsections": [],
    "raw_text": "tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\nthere more principled ways to do it? 4) Finally, the rank-de\ufb01ciency of \u0001Wsuggests that Wcould\nbe rank-de\ufb01cient as well, which can also be a source of inspiration for future works.\nREFERENCES\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\nhttp://arxiv.org/abs/2012.13255 .\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Ef\ufb01ciently, Going Beyond Kernels? In\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puri\ufb01cation: How adversarial training performs robust\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\n03962 .\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\nmatrix completion. SIAM Journal on optimization , 20(4):1956\u20131982, 2010.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\nRonan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: deep\nneural networks with multitask learning. In Proceedings of the 25th international conference\non Machine learning , ICML \u201908, pp. 160\u2013167, New York, NY , USA, July 2008. Association\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\nhttps://doi.org/10.1145/1390156.1390177 .\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning, 2014.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019a.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\nhttps://aclanthology.org/I05-5002 .\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\nNatural Language Generation , pp. 124\u2013133, 2017.\n13"
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 14,
    "ending_page_no": 14,
    "entities": [],
    "subsections": [],
    "raw_text": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\nhttp://arxiv.org/abs/1911.12237 .\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\napproximation techniques. GAMM-Mitteilungen , 36(1):53\u201378, 2013.\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\nlearning. In ICML , pp. 376\u2013383, 2008. URL https://doi.org/10.1145/1390156.\n1390204 .\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\n2101.00121 . arXiv: 2101.00121.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention, 2021.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Ef\ufb01cient Transfer Learning\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\n00751 .\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\nof factorized neural layers, 2021.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding, 2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Ef\ufb01cient Prompt\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\narXiv: 2104.08691.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\nXiang Lisa Li and Percy Liang. Pre\ufb01x-Tuning: Optimizing Continuous Prompts for Generation.\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\n2358\u20132367. PMLR, 2016.\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\nory, pp. 2\u201347. PMLR, 2018b.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\nvia parameter-ef\ufb01cient transfer learning. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pp. 441\u2013459, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.\ufb01ndings-emnlp.41. URL https://aclanthology.\norg/2020.findings-emnlp.41 .\n14"
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 15,
    "ending_page_no": 15,
    "entities": [],
    "subsections": [],
    "raw_text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\n2103.10385 . arXiv: 2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Ef\ufb01cient low-rank\nhypercomplex adapter layers, 2021.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.\nJekaterina Novikova, Ond \u02c7rej Du \u02c7sek, and Verena Rieser. The e2e dataset: New challenges for end-\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\narXiv:1906.05392 , 2019.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R \u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning, 2021.\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\nInterspeech , pp. 3743\u20133747, 2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\nstanding by Generative Pre-Training. pp. 12, a.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. pp. 24, b.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\nSylvestre-Alvise Rebuf\ufb01, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\nabs/1705.08045 . arXiv: 1705.08045.\nAndreas R \u00a8uckl\u00b4e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. Adapterdrop: On the ef\ufb01ciency of adapters in transformers, 2020.\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-dimensional output targets.\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655\u2013\n6659. IEEE, 2013.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism, 2020.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing , pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\n15"
  },
  {
    "subject_title": "References",
    "section_type": "References",
    "starting_page_no": 16,
    "ending_page_no": 16,
    "entities": [
      {
        "author": "Ashish Vaswani"
      },
      {
        "author": "Alex Wang"
      },
      {
        "author": "Alex Warstadt"
      },
      {
        "author": "Adina Williams"
      },
      {
        "author": "Thomas Wolf"
      },
      {
        "author": "Greg Yang"
      },
      {
        "author": "Elad Ben Zaken"
      },
      {
        "author": "Yu Zhang"
      },
      {
        "author": "Yong Zhao"
      },
      {
        "author": "Victor Zhong"
      },
      {
        "publication year": "2017"
      },
      {
        "publication year": "2018"
      },
      {
        "publication year": "2019"
      },
      {
        "publication year": "2020"
      },
      {
        "publication year": "2021"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES",
    "section_type": "Section Header",
    "starting_page_no": 16,
    "ending_page_no": 16,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "author": "Brown et al."
      },
      {
        "publication year": "2020"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Method",
    "section_type": "Methodology",
    "starting_page_no": 17,
    "ending_page_no": 17,
    "entities": [
      {
        "company name": "None",
        "publication year": "2020",
        "model name": "GPT-2 medium"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "B I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS",
    "section_type": "Discussion",
    "starting_page_no": 17,
    "ending_page_no": 17,
    "entities": [
      {
        "author name": "Lin",
        "publication year": "2019"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "C D ATASET DETAILS",
    "section_type": "Dataset Description",
    "starting_page_no": 17,
    "ending_page_no": 17,
    "entities": [
      {
        "dataset name": "RTE"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Datasets",
    "section_type": "Section",
    "starting_page_no": 18,
    "ending_page_no": 18,
    "entities": [
      {
        "dataset": "GLUE"
      },
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "SAMSum"
      },
      {
        "dataset": "E2E NLG Challenge"
      },
      {
        "dataset": "DART"
      },
      {
        "dataset": "WebNLG"
      },
      {
        "publication year": "2017"
      },
      {
        "publication year": "2019"
      },
      {
        "publication year": "2020"
      },
      {
        "license": "BSD 3-Clause License"
      },
      {
        "license": "Creative Commons BY-NC-ND 4.0"
      },
      {
        "license": "Creative Commons BY-NC-SA 4.0"
      },
      {
        "license": "MIT license"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "D HYPERPARAMETERS USED IN EXPERIMENTS",
    "section_type": "Section",
    "starting_page_no": 18,
    "ending_page_no": 18,
    "entities": [
      {
        "model": "RoBERTa"
      },
      {
        "model": "DeBERTa"
      },
      {
        "publication year": "2019"
      },
      {
        "publication year": "2021"
      },
      {
        "author": "Liu et al."
      },
      {
        "author": "Houlsby et al."
      },
      {
        "author": "Pfeiffer et al."
      },
      {
        "author": "He et al."
      }
    ],
    "subsections": [
      {
        "subject_title": "D.1 ROBERTA",
        "section_type": "Subsection",
        "starting_page_no": 18,
        "ending_page_no": 18,
        "entities": [],
        "subsections": []
      },
      {
        "subject_title": "D.2 DEBERTA",
        "section_type": "Subsection",
        "starting_page_no": 18,
        "ending_page_no": 18,
        "entities": [],
        "subsections": []
      }
    ]
  },
  {
    "subject_title": "Hyperparameters for RoBERTa on GLUE benchmark",
    "section_type": "Table",
    "starting_page_no": 19,
    "ending_page_no": 19,
    "entities": [
      {
        "model": "RoBERTa"
      },
      {
        "benchmark": "GLUE"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "GPT-2",
    "section_type": "Model Description",
    "starting_page_no": 19,
    "ending_page_no": 19,
    "entities": [
      {
        "model": "GPT-2"
      },
      {
        "authors": "Loshchilov & Hutter"
      },
      {
        "year": "2017"
      },
      {
        "authors": "Li & Liang"
      },
      {
        "year": "2021"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "GPT-3",
    "section_type": "Model Description",
    "starting_page_no": 19,
    "ending_page_no": 19,
    "entities": [
      {
        "model": "GPT-3"
      },
      {
        "authors": "Loshchilov & Hutter"
      },
      {
        "year": "2017"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Hyperparameters for DeBERTa XXL on GLUE benchmark",
    "section_type": "Table",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "dataset": "MNLI"
      },
      {
        "dataset": "SST-2"
      },
      {
        "dataset": "MRPC"
      },
      {
        "dataset": "CoLA"
      },
      {
        "dataset": "QNLI"
      },
      {
        "dataset": "QQP"
      },
      {
        "dataset": "RTE"
      },
      {
        "dataset": "STS-B"
      },
      {
        "model": "DeBERTa XXL"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART",
    "section_type": "Table",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "dataset": "E2E"
      },
      {
        "dataset": "WebNLG"
      },
      {
        "dataset": "DART"
      },
      {
        "model": "GPT-2 LoRA"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Hyperparameter Tuning",
    "section_type": "Paragraph",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "paper": "Zhong et al., 2017"
      },
      {
        "paper": "Williams et al., 2018"
      },
      {
        "paper": "Gliwa et al., 2019"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Combining LoRA with Prefix Tuning",
    "section_type": "Section Header",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "model": "LoRA"
      },
      {
        "model": "GPT-3"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "LoRA+PrefixEmbed (LoRA+PE)",
    "section_type": "Subsection",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "MNLI"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "LoRA+PrefixLayer (LoRA+PL)",
    "section_type": "Subsection",
    "starting_page_no": 20,
    "ending_page_no": 20,
    "entities": [
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "MNLI"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Hyperparameters",
    "section_type": "Table Caption",
    "starting_page_no": 21,
    "ending_page_no": 21,
    "entities": [
      {
        "optimizer": "AdamW"
      },
      {
        "batch size": 128
      },
      {
        "epoch": 2
      },
      {
        "warmup tokens": 250000
      },
      {
        "learning rate": [
          "5.00E-06",
          "5.00E-04",
          "1.00E-04",
          "1.6E-03",
          "1.00E-04",
          "2.00E-04"
        ]
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "F ADDITIONAL EMPIRICAL EXPERIMENTS",
    "section_type": "Section Header",
    "starting_page_no": 21,
    "ending_page_no": 21,
    "entities": [
      {
        "dataset": "WikiSQL"
      },
      {
        "dataset": "MultiNLI"
      },
      {
        "dataset": "DART"
      },
      {
        "dataset": "WebNLG"
      },
      {
        "year": "2020"
      },
      {
        "year": "2017"
      },
      {
        "year": "2021"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "F.1 ADDITIONAL EXPERIMENTS ON GPT-2",
    "section_type": "Subsection Header",
    "starting_page_no": 21,
    "ending_page_no": 21,
    "entities": [
      {
        "model": "GPT-2 Medium"
      },
      {
        "model": "GPT-2 Large"
      },
      {
        "method": "Fine-Tune"
      },
      {
        "method": "AdapterL0"
      },
      {
        "method": "AdapterL1"
      },
      {
        "method": "FTTop2"
      },
      {
        "method": "PrefLayer"
      },
      {
        "method": "LoRA"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Method WebNLG",
    "section_type": "Table",
    "starting_page_no": 22,
    "ending_page_no": 22,
    "entities": [
      {
        "model": "GPT-2 Medium"
      },
      {
        "model": "GPT-2 Large"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "F.2 ADDITIONAL EXPERIMENTS ON GPT-3",
    "section_type": "Section Header",
    "starting_page_no": 22,
    "ending_page_no": 22,
    "entities": [
      {
        "model": "GPT-3"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "F.3 LOW-DATA REGIME",
    "section_type": "Section Header",
    "starting_page_no": 22,
    "ending_page_no": 22,
    "entities": [
      {
        "dataset": "MNLI"
      },
      {
        "dataset": "MNLI-n"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "G MEASURING SIMILARITY BETWEEN SUBSPACES",
    "section_type": "Section Header",
    "starting_page_no": 22,
    "ending_page_no": 22,
    "entities": [
      {
        "paper": "Ham & Lee (2008)"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Method Hyperparameters",
    "section_type": "Table",
    "starting_page_no": 23,
    "ending_page_no": 23,
    "entities": [
      {
        "WikiSQL": "dataset"
      },
      {
        "MNLI-m": "dataset"
      },
      {
        "GPT-3": "model"
      },
      {
        "175B": "model size"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Hyperparameter analysis",
    "section_type": "Paragraph",
    "starting_page_no": 23,
    "ending_page_no": 23,
    "entities": [
      {
        "LoRA": "adaptation approach"
      },
      {
        "Pre\ufb01xEmbed": "adaptation approach"
      },
      {
        "Pre\ufb01xLayer": "adaptation approach"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Validation accuracy of different methods",
    "section_type": "Table",
    "starting_page_no": 23,
    "ending_page_no": 23,
    "entities": [
      {
        "GPT-3": "model"
      },
      {
        "MNLI": "dataset"
      },
      {
        "175B": "model size"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Projection Metric",
    "section_type": "Mathematical Definition",
    "starting_page_no": 23,
    "ending_page_no": 23,
    "entities": [
      {
        "Ham & Lee": "authors"
      },
      {
        "2008": "publication year"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": null,
    "section_type": "No Section Detected",
    "starting_page_no": 24,
    "ending_page_no": 24,
    "entities": [],
    "subsections": [],
    "raw_text": "Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\nOptimizer - AdamW\nWarmup Tokens - 250,000\nLR Schedule - Linear\nBatch Size - 20 20 100 128\n# Epoch - 40 40 4 2\nLearning RateFineTune 5.00E-6\nPre\ufb01xEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\nPre\ufb01xLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\nLoRA 2.00E-4\nPre\ufb01xEmbed lp 16 32 64 256\nAdaptation- Pre\ufb01xEmbed li 8\nSpeci\ufb01c Pre\ufb01xTune lp=li= 8\nLoRA rq=rv= 8\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\nwhere our similarity is de\ufb01ned as:\n\u001e(A;B;i;j ) = (Ui\nA;Uj\nB) =Pp\ni=1\u001b2\ni\np=1\np\u0010\n1\u0000d(Ui\nA;Uj\nB)2\u0011\nThis similarity satis\ufb01es that if Ui\nAandUj\nBshare the same column span, then \u001e(A;B;i;j ) = 1 . If\nthey are completely orthogonal, then \u001e(A;B;i;j ) = 0 . Otherwise, \u001e(A;B;i;j )2(0;1).\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\nWe present additional results from our investigation into the low-rank update matrices.\nH.1 C ORRELATION BETWEEN LORA M ODULES\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\nlayers.\nH.2 E FFECT OFrONGPT-2\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\ndataset as an example, we report the validation loss and test metrics achieved by different choices\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\nNote that the relationship between model size and the optimal rank for adaptation is still an open\nquestion.\nH.3 C ORRELATION BETWEEN WAND \u0001W\nSee Figure 8 for the normalized subspace similarity between Wand\u0001Wwith varying r.\nNote again that \u0001Wdoes not contain the top singular directions of W, since the similarity between\nthe top 4 directions in \u0001Wand the top-10% of those in Wbarely exceeds 0.2. This gives evidence\nthat\u0001Wcontains those \u201ctask-speci\ufb01c\u201d directions that are otherwise notemphasized in W.\nAn interesting next question to answer, is how \u201cstrong\u201d do we need to amplify those task-speci\ufb01c\ndirections, in order for the model adaptation to work well?\n24"
  },
  {
    "subject_title": "Figure 6: Normalized subspace similarity",
    "section_type": "Figure",
    "starting_page_no": 25,
    "ending_page_no": 25,
    "entities": [],
    "subsections": []
  },
  {
    "subject_title": "H.4 Amplification Factor",
    "section_type": "Section",
    "starting_page_no": 25,
    "ending_page_no": 25,
    "entities": [
      {
        "amplification factor": 20
      },
      {
        "amplification factor": 2
      },
      {
        "rank": 4
      },
      {
        "rank": 64
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Figure 7",
    "section_type": "Image",
    "starting_page_no": 26,
    "ending_page_no": 26,
    "entities": [
      {
        "layer": "96",
        "model": "Transformer"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Table 18",
    "section_type": "Table",
    "starting_page_no": 26,
    "ending_page_no": 26,
    "entities": [
      {
        "model": "GPT-2 Medium",
        "challenge": "E2E NLG Challenge"
      }
    ],
    "subsections": []
  },
  {
    "subject_title": "Figure 8",
    "section_type": "Image",
    "starting_page_no": 26,
    "ending_page_no": 26,
    "entities": [
      {
        "model": "Wq",
        "baseline": "random"
      }
    ],
    "subsections": []
  }
]