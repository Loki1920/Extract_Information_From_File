{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install & Import Dependencies\n"
      ],
      "metadata": {
        "id": "F62xbPk9zvoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nxMxOBXEzrPp"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai PyPDF2 python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import dependencies"
      ],
      "metadata": {
        "id": "lnjh-t-Ez88Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import PyPDF2\n",
        "import docx\n",
        "from google.colab import files\n",
        "import os"
      ],
      "metadata": {
        "id": "ReHnRLTUz5f2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Document Upload"
      ],
      "metadata": {
        "id": "Hi15m1ko0IMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document upload\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "file_ext = os.path.splitext(filename)[1].lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9wtHuvqp0DaV",
        "outputId": "64ffd8d7-64ff-469f-c174-5033d090e182"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9896d980-1876-4b70-b38b-aad2b5dd4fe2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9896d980-1876-4b70-b38b-aad2b5dd4fe2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sample_document.pdf to sample_document.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract Text from PDF, DOCX, or TXT"
      ],
      "metadata": {
        "id": "XrdSlblj0XeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(filename):\n",
        "    # Get the file extension in lowercase (e.g., '.pdf', '.docx', '.txt')\n",
        "    ext = os.path.splitext(filename)[1].lower()\n",
        "    pages = []  # List to store extracted text, organized by page or as a single entry\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        # If the file is a PDF, use PyPDF2 to read it\n",
        "        pdf_reader = PyPDF2.PdfReader(open(filename, \"rb\"))\n",
        "        for i, page in enumerate(pdf_reader.pages):\n",
        "            text = page.extract_text()  # Extract text from each page\n",
        "            pages.append({\"page_num\": i+1, \"text\": text})  # Store with page number\n",
        "\n",
        "    elif ext == \".docx\":\n",
        "        # If the file is a DOCX, use python-docx to read it\n",
        "        doc = docx.Document(filename)\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            full_text.append(para.text)  # Collect text from each paragraph\n",
        "        # Combine all paragraphs into one string and treat as a single \"page\"\n",
        "        pages.append({\"page_num\": 1, \"text\": \"\\n\".join(full_text)})\n",
        "\n",
        "    elif ext == \".txt\":\n",
        "        # If the file is a TXT, read the whole file as one \"page\"\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        pages.append({\"page_num\": 1, \"text\": text})\n",
        "\n",
        "    else:\n",
        "        # Raise an error if the file type is not supported\n",
        "        raise ValueError(\"Unsupported file type: \" + ext)\n",
        "\n",
        "    return pages  # Return the list of extracted text entries\n",
        "\n",
        "pages = extract_text(filename)"
      ],
      "metadata": {
        "id": "rLwNugca0O4q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LLM-based Parsing"
      ],
      "metadata": {
        "id": "CwsVNWE90ks1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \" \"   # use your api_key,hide for production"
      ],
      "metadata": {
        "id": "ZiuppVgh0cro"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an OpenAI client instance for interacting with the Groq API.\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=groq_api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "VtP2vAgY0vyV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sections_with_llm_chunked_all_pages(pages):\n",
        "    \"\"\"\n",
        "    Processes each page individually with the LLM to avoid token limit errors,\n",
        "    and ensures every page is represented in the output, even if no section is detected.\n",
        "    \"\"\"\n",
        "    all_sections = []\n",
        "\n",
        "    for page in pages:\n",
        "        # If the page is empty, still add a placeholder\n",
        "        if not page['text'] or not page['text'].strip():\n",
        "            all_sections.append({\n",
        "                \"subject_title\": None,\n",
        "                \"section_type\": \"No Content\",\n",
        "                \"starting_page_no\": page['page_num'],\n",
        "                \"ending_page_no\": page['page_num'],\n",
        "                \"entities\": [],\n",
        "                \"subsections\": [],\n",
        "                \"raw_text\": \"\"  # Optionally include raw text\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Prepare a prompt for just this page\n",
        "        prompt = f\"\"\"\n",
        "You are a document parser. Given the following page, extract its main sections and for each section, provide:\n",
        "- subject_title (section header)\n",
        "- section_type (guess if not explicit)\n",
        "- starting_page_no\n",
        "- ending_page_no\n",
        "- entities (list of key named entities, e.g. company names, dates, financial figures, etc.)\n",
        "- subsections (leave empty for now)\n",
        "\n",
        "Format your output as a JSON list, as in this example:\n",
        "[\n",
        "  {{\n",
        "    \"subject_title\": \"<section_header>\",\n",
        "    \"section_type\": \"Document Title\",\n",
        "    \"starting_page_no\": 1,\n",
        "    \"ending_page_no\": 1,\n",
        "    \"entities\": [{{\"company name\": \"Amazon\", \"publication year\": \"2024\"}}],\n",
        "    \"subsections\": []\n",
        "  }},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Page {page['page_num']}:\n",
        "{page['text']}\n",
        "\"\"\"\n",
        "\n",
        "        # Send the prompt to the LLM and get the response\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "\n",
        "        import re, ast, json\n",
        "        content = response.choices[0].message.content\n",
        "        try:\n",
        "            # Try to extract the JSON list from the response using regex\n",
        "            json_str = re.search(r'(\\[.*\\])', content, re.DOTALL).group(1)\n",
        "            parsed = json.loads(json_str)\n",
        "        except Exception:\n",
        "            try:\n",
        "                parsed = ast.literal_eval(content)\n",
        "            except Exception:\n",
        "                parsed = []\n",
        "\n",
        "        # If the LLM found sections, add them; otherwise, add a placeholder\n",
        "        if isinstance(parsed, list) and parsed:\n",
        "            all_sections.extend(parsed)\n",
        "        else:\n",
        "            all_sections.append({\n",
        "                \"subject_title\": None,\n",
        "                \"section_type\": \"No Section Detected\",\n",
        "                \"starting_page_no\": page['page_num'],\n",
        "                \"ending_page_no\": page['page_num'],\n",
        "                \"entities\": [],\n",
        "                \"subsections\": [],\n",
        "                \"raw_text\": page['text']  # Optionally include raw text\n",
        "            })\n",
        "\n",
        "    return all_sections  # Return the aggregated list of all sections/pages"
      ],
      "metadata": {
        "id": "KO0Jl74QTc65"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_json = parse_sections_with_llm_chunked_all_pages(pages)"
      ],
      "metadata": {
        "id": "pXohfiMA1kcP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(structured_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfhoGr9S2p9T",
        "outputId": "3fec0b94-72da-48d9-84bd-c73273693445"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'subject_title': 'LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS', 'section_type': 'Document Title', 'starting_page_no': 1, 'ending_page_no': 1, 'entities': [{'company name': 'Microsoft Corporation'}, {'authors': ['Edward Hu', 'Yelong Shen', 'Phillip Wallis', 'Zeyuan Allen-Zhu', 'Yuanzhi Li', 'Shean Wang', 'Lu Wang', 'Weizhu Chen']}], 'subsections': []}, {'subject_title': 'ABSTRACT', 'section_type': 'Abstract', 'starting_page_no': 1, 'ending_page_no': 1, 'entities': [{'model name': 'GPT-3'}, {'parameter count': '175B'}, {'company name': 'Microsoft'}], 'subsections': []}, {'subject_title': 'INTRODUCTION', 'section_type': 'Section', 'starting_page_no': 1, 'ending_page_no': 1, 'entities': [{'model names': ['RoBERTa', 'DeBERTa', 'GPT-2', 'GPT-3']}, {'authors': ['Radford et al.', 'Liu et al.', 'Brown et al.']}, {'publication years': ['b', '2019', '2020']}], 'subsections': []}, {'subject_title': 'Introduction', 'section_type': 'Abstract', 'starting_page_no': 2, 'ending_page_no': 2, 'entities': [{'author': 'Houlsby et al.'}, {'author': 'Rebufﬁ et al.'}, {'author': 'Li et al.'}, {'author': 'Aghajanyan et al.'}, {'model': 'GPT-3'}, {'publication year': '2019'}, {'publication year': '2017'}, {'publication year': '2021'}, {'publication year': '2020'}, {'publication year': '2018'}], 'subsections': []}, {'subject_title': 'Terminologies and Conventions', 'section_type': 'Definition', 'starting_page_no': 2, 'ending_page_no': 2, 'entities': [{'model': 'Transformer'}, {'author': 'Vaswani et al.'}, {'author': 'Brown et al.'}, {'author': 'Loshchilov & Hutter'}, {'author': 'Kingma & Ba'}], 'subsections': []}, {'subject_title': 'PROBLEM STATEMENT', 'section_type': 'Problem Definition', 'starting_page_no': 2, 'ending_page_no': 2, 'entities': [{'model': 'GPT'}, {'author': 'Radford et al.'}, {'author': 'Brown et al.'}, {'author': 'Vaswani et al.'}], 'subsections': []}, {'subject_title': 'Full Fine-Tuning', 'section_type': 'Introduction', 'starting_page_no': 3, 'ending_page_no': 3, 'entities': [{'model': 'GPT-3', 'parameter count': '175Billion'}], 'subsections': []}, {'subject_title': 'Are Existing Solutions Good Enough?', 'section_type': 'Related Work', 'starting_page_no': 3, 'ending_page_no': 3, 'entities': [{'model': 'GPT-2', 'authors': ['Houlsby et al.', 'Rebufﬁ et al.', 'Pfeiffer et al.', 'R ¨uckl´e et al.', 'Li & Liang', 'Lester et al.', 'Hambardzumyan et al.', 'Liu et al.', 'Lin et al.', 'Ba et al.', 'Radford et al.', 'Shoeybi et al.', 'Lepikhin et al.']}], 'subsections': []}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 4, 'ending_page_no': 4, 'entities': [], 'subsections': [], 'raw_text': 'Batch Size 32 16 1\\nSequence Length 512 256 128\\nj\\x02j 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4\\x060.8 338.0 \\x060.6 19.8 \\x062.7\\nAdapterL1482.0\\x061.0 (+2.2%) 354.8 \\x060.5 (+5.0%) 23.9 \\x062.1 (+20.7%)\\nAdapterH1492.2\\x061.0 (+3.0%) 366.3 \\x060.5 (+8.4%) 25.8 \\x062.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. “ j\\x02j” denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\\nlearn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\\nweight matrix W02Rd\\x02k, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ \\x01W=W0+BA, whereB2Rd\\x02r;A2Rr\\x02k, and the rank r\\x1cmin(d;k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and\\x01W=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiﬁed forward pass yields:\\nh=W0x+ \\x01Wx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so\\x01W=BAis zero at the beginning of training. We then scale \\x01Wx by\\x0b\\nr, where\\x0b\\nis a constant in r. When optimizing with Adam, tuning \\x0bis roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set \\x0bto the ﬁrstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full ﬁne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preﬁx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\\x02k.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B0A0, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4'}, {'subject_title': 'APPLYING LORA TO TRANSFORMER', 'section_type': 'Methodology', 'starting_page_no': 5, 'ending_page_no': 5, 'entities': [{'model': 'Transformer'}, {'module': 'self-attention'}, {'module': 'MLP'}, {'parameter': 'Wq'}, {'parameter': 'Wk'}, {'parameter': 'Wv'}, {'parameter': 'Wo'}], 'subsections': []}, {'subject_title': 'PRACTICAL BENEFITS AND LIMITATIONS', 'section_type': 'Advantages and Limitations', 'starting_page_no': 5, 'ending_page_no': 5, 'entities': [{'model': 'GPT-3 175B'}, {'metric': 'VRAM usage'}, {'metric': 'checkpoint size'}, {'metric': 'speedup'}], 'subsections': []}, {'subject_title': 'EMPIRICAL EXPERIMENTS', 'section_type': 'Experiments', 'starting_page_no': 5, 'ending_page_no': 5, 'entities': [{'model': 'RoBERTa'}, {'model': 'DeBERTa'}, {'model': 'GPT-2'}, {'model': 'GPT-3 175B'}, {'dataset': 'GLUE'}, {'dataset': 'WikiSQL'}, {'dataset': 'SAMSum'}], 'subsections': [{'subject_title': 'BASELINES', 'section_type': 'Sub-experiment', 'starting_page_no': 5, 'ending_page_no': 5, 'entities': [{'baseline': 'Fine-Tuning (FT)'}, {'baseline': 'FTTop2'}], 'subsections': []}]}, {'subject_title': 'Model & Method', 'section_type': 'Table', 'starting_page_no': 6, 'ending_page_no': 6, 'entities': [{'model': 'RoBERTa base'}, {'model': 'RoBERTa large'}, {'model': 'DeBERTa XXL'}, {'parameter': '125.0M'}, {'parameter': '0.1M'}, {'parameter': '0.3M'}, {'parameter': '0.9M'}, {'parameter': '355.0M'}, {'parameter': '0.8M'}, {'parameter': '3.0M'}, {'parameter': '6.0M'}, {'parameter': '1500.0M'}, {'parameter': '4.7M'}], 'subsections': []}, {'subject_title': 'Description of Adaptation Methods', 'section_type': 'Text', 'starting_page_no': 6, 'ending_page_no': 6, 'entities': [{'author': 'Houlsby et al.'}, {'author': 'Zaken et al.'}, {'author': 'Li & Liang'}, {'author': 'Lin et al.'}, {'author': 'Pfeiffer et al.'}, {'author': 'Rückl´e et al.'}], 'subsections': []}, {'subject_title': 'Model & Method', 'section_type': 'Experiment Results', 'starting_page_no': 7, 'ending_page_no': 7, 'entities': [{'model': 'GPT-2 M'}, {'model': 'GPT-2 L'}, {'metric': 'BLEU'}, {'metric': 'NIST'}, {'metric': 'MET'}, {'metric': 'ROUGE-L'}, {'metric': 'CIDEr'}], 'subsections': []}, {'subject_title': '5.2 ROBERTA BASE/LARGE', 'section_type': 'Model Description', 'starting_page_no': 7, 'ending_page_no': 7, 'entities': [{'model': 'RoBERTa'}, {'author': 'Liu et al.'}, {'year': '2019'}, {'model': 'BERT'}, {'author': 'Devlin et al.'}, {'year': '2019'}], 'subsections': []}, {'subject_title': '5.3 DeBERTa AXXL', 'section_type': 'Model Description', 'starting_page_no': 7, 'ending_page_no': 7, 'entities': [{'model': 'DeBERTa'}, {'author': 'He et al.'}, {'year': '2021'}], 'subsections': []}, {'subject_title': '5.4 GPT-2 MEDIUM/LARGE', 'section_type': 'Experiment Description', 'starting_page_no': 7, 'ending_page_no': 7, 'entities': [{'model': 'GPT-2 medium'}, {'model': 'GPT-2 large'}, {'author': 'Radford et al.'}, {'year': 'b'}], 'subsections': []}, {'subject_title': 'Model & Method', 'section_type': 'Table', 'starting_page_no': 8, 'ending_page_no': 8, 'entities': [{'model': 'GPT-3'}, {'dataset': 'WikiSQL'}, {'dataset': 'MultiNLI-matched'}, {'dataset': 'SAMSum'}], 'subsections': []}, {'subject_title': 'Scaling up to GPT-3 175B', 'section_type': 'Text', 'starting_page_no': 8, 'ending_page_no': 8, 'entities': [{'model': 'GPT-3'}, {'parameter count': '175 billion'}], 'subsections': []}, {'subject_title': 'Figure 2: GPT-3 175B validation accuracy', 'section_type': 'Figure', 'starting_page_no': 8, 'ending_page_no': 8, 'entities': [{'model': 'GPT-3'}, {'dataset': 'WikiSQL'}, {'dataset': 'MultiNLI-matched'}], 'subsections': []}, {'subject_title': 'Related Works', 'section_type': 'Section', 'starting_page_no': 8, 'ending_page_no': 8, 'entities': [{'model': 'Transformer'}, {'paper': 'Vaswani et al. (2017)'}, {'paper': 'Radford et al. (a)'}, {'paper': 'Devlin et al. (2019b)'}, {'paper': 'Radford et al. (b)'}], 'subsections': []}, {'subject_title': 'Transformer Language Models', 'section_type': 'Introduction', 'starting_page_no': 9, 'ending_page_no': 9, 'entities': [{'company name': 'None'}, {'publication year': '2020'}, {'model name': 'GPT-3'}, {'parameter count': '175B'}], 'subsections': []}, {'subject_title': 'Prompt Engineering and Fine-Tuning', 'section_type': 'Subsection', 'starting_page_no': 9, 'ending_page_no': 9, 'entities': [{'model name': 'GPT-3 175B'}, {'publication year': '2020'}], 'subsections': []}, {'subject_title': 'Parameter-Efficient Adaptation', 'section_type': 'Subsection', 'starting_page_no': 9, 'ending_page_no': 9, 'entities': [{'model name': 'COMPACTER'}, {'publication year': '2021'}], 'subsections': []}, {'subject_title': 'Low-Rank Structures in Deep Learning', 'section_type': 'Subsection', 'starting_page_no': 9, 'ending_page_no': 9, 'entities': [{'publication year': '2016'}, {'publication year': '2010'}, {'publication year': '2018'}, {'publication year': '2013'}], 'subsections': []}, {'subject_title': 'UNDERSTANDING THE LOW-RANK UPDATES', 'section_type': 'Section', 'starting_page_no': 9, 'ending_page_no': 9, 'entities': [{'model name': 'LoRA'}, {'model name': 'GPT-3 175B'}], 'subsections': []}, {'subject_title': '7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?', 'section_type': 'Section', 'starting_page_no': 10, 'ending_page_no': 10, 'entities': [{'model': 'GPT-3 175B'}, {'parameter budget': '18M'}, {'storage size': '35MB'}], 'subsections': []}, {'subject_title': '7.2 WHAT IS THE OPTIMAL RANKrFOR LORA?', 'section_type': 'Section', 'starting_page_no': 10, 'ending_page_no': 10, 'entities': [{'model': 'GPT-2'}], 'subsections': []}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 11, 'ending_page_no': 11, 'entities': [], 'subsections': [], 'raw_text': 'Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\\x14i\\x148) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1\\x14j\\x1464)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\n\\x1e(Ar=8;Ar=64;i;j) =jjUi>\\nAr=8Uj\\nAr=64jj2\\nF\\nmin(i;j)2[0;1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\n\\x1e(\\x01)has a range of [0;1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how \\x1echanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both \\x01Wqand\\x01Wv.\\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiﬁcantly between\\nAr=8andAr=64, while others do not. Speciﬁcally, \\x01Wv(resp. \\x01Wq) ofAr=8\\nand\\x01Wv(resp. \\x01Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0:5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conﬁrm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n\\x01Wqappears to have a higher “intrinsic rank” than \\x01Wv, since more common singular value direc-\\ntions are learned by both runs for \\x01Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX \\x01WCOMPARE TO W?\\nWe further investigate the relationship between \\x01WandW. In particular, does \\x01Whighly correlate\\nwithW? (Or mathematically, is \\x01Wmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices – we stick with\\nAfor our experiments.\\n11'}, {'subject_title': 'Figure 4', 'section_type': 'Figure', 'starting_page_no': 12, 'ending_page_no': 12, 'entities': [{'model': 'GPT-3'}, {'layer': '48-th layer'}], 'subsections': []}, {'subject_title': 'Table 7', 'section_type': 'Table', 'starting_page_no': 12, 'ending_page_no': 12, 'entities': [{'model': 'GPT-3'}, {'layer': '48th layer'}], 'subsections': []}, {'subject_title': 'CONCLUSION AND FUTURE WORK', 'section_type': 'Conclusion', 'starting_page_no': 12, 'ending_page_no': 12, 'entities': [{'model': 'Transformer language models'}], 'subsections': []}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 13, 'ending_page_no': 13, 'entities': [], 'subsections': [], 'raw_text': 'tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deﬁciency of \\x01Wsuggests that Wcould\\nbe rank-deﬁcient as well, which can also be a source of inspiration for future works.\\nREFERENCES\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\\nhttp://arxiv.org/abs/2012.13255 .\\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Kernels? In\\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\\n03962 .\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\\nmatrix completion. SIAM Journal on optimization , 20(4):1956–1982, 2010.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep\\nneural networks with multitask learning. In Proceedings of the 25th international conference\\non Machine learning , ICML ’08, pp. 160–167, New York, NY , USA, July 2008. Association\\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\\nhttps://doi.org/10.1145/1390156.1390177 .\\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\\nparameters in deep learning, 2014.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019a.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\\nhttps://aclanthology.org/I05-5002 .\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\\nNatural Language Generation , pp. 124–133, 2017.\\n13'}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 14, 'ending_page_no': 14, 'entities': [], 'subsections': [], 'raw_text': 'Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\\nhttp://arxiv.org/abs/1911.12237 .\\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\\napproximation techniques. GAMM-Mitteilungen , 36(1):53–78, 2013.\\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\\nlearning. In ICML , pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.\\n1390204 .\\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\\n2101.00121 . arXiv: 2101.00121.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning\\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\\n00751 .\\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\\nof factorized neural layers, 2021.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt\\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\\narXiv: 2104.08691.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\\nXiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation.\\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\\n2358–2367. PMLR, 2016.\\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\\nory, pp. 2–47. PMLR, 2018b.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020 , pp. 441–459, Online, November 2020. Association for Computational\\nLinguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.41. URL https://aclanthology.\\norg/2020.findings-emnlp.41 .\\n14'}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 15, 'ending_page_no': 15, 'entities': [], 'subsections': [], 'raw_text': 'Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\\n2103.10385 . arXiv: 2103.10385.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank\\nhypercomplex adapter layers, 2021.\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.\\nJekaterina Novikova, Ond ˇrej Du ˇsek, and Verena Rieser. The e2e dataset: New challenges for end-\\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\\narXiv:1906.05392 , 2019.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\\nInterspeech , pp. 3743–3747, 2018.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\\nstanding by Generative Pre-Training. pp. 12, a.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nModels are Unsupervised Multitask Learners. pp. 24, b.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\\nabs/1705.08045 . arXiv: 1705.08045.\\nAndreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\\nIryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers, 2020.\\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\\nrank matrix factorization for deep neural network training with high-dimensional output targets.\\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655–\\n6659. IEEE, 2013.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\\nallelism, 2020.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\\nProcessing , pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\\n15'}, {'subject_title': 'References', 'section_type': 'References', 'starting_page_no': 16, 'ending_page_no': 16, 'entities': [{'author': 'Ashish Vaswani'}, {'author': 'Alex Wang'}, {'author': 'Alex Warstadt'}, {'author': 'Adina Williams'}, {'author': 'Thomas Wolf'}, {'author': 'Greg Yang'}, {'author': 'Elad Ben Zaken'}, {'author': 'Yu Zhang'}, {'author': 'Yong Zhao'}, {'author': 'Victor Zhong'}, {'publication year': '2017'}, {'publication year': '2018'}, {'publication year': '2019'}, {'publication year': '2020'}, {'publication year': '2021'}], 'subsections': []}, {'subject_title': 'A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES', 'section_type': 'Section Header', 'starting_page_no': 16, 'ending_page_no': 16, 'entities': [{'model': 'GPT-3'}, {'author': 'Brown et al.'}, {'publication year': '2020'}], 'subsections': []}, {'subject_title': 'Method', 'section_type': 'Methodology', 'starting_page_no': 17, 'ending_page_no': 17, 'entities': [{'company name': 'None', 'publication year': '2020', 'model name': 'GPT-2 medium'}], 'subsections': []}, {'subject_title': 'B I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS', 'section_type': 'Discussion', 'starting_page_no': 17, 'ending_page_no': 17, 'entities': [{'author name': 'Lin', 'publication year': '2019'}], 'subsections': []}, {'subject_title': 'C D ATASET DETAILS', 'section_type': 'Dataset Description', 'starting_page_no': 17, 'ending_page_no': 17, 'entities': [{'dataset name': 'RTE'}], 'subsections': []}, {'subject_title': 'Datasets', 'section_type': 'Section', 'starting_page_no': 18, 'ending_page_no': 18, 'entities': [{'dataset': 'GLUE'}, {'dataset': 'WikiSQL'}, {'dataset': 'SAMSum'}, {'dataset': 'E2E NLG Challenge'}, {'dataset': 'DART'}, {'dataset': 'WebNLG'}, {'publication year': '2017'}, {'publication year': '2019'}, {'publication year': '2020'}, {'license': 'BSD 3-Clause License'}, {'license': 'Creative Commons BY-NC-ND 4.0'}, {'license': 'Creative Commons BY-NC-SA 4.0'}, {'license': 'MIT license'}], 'subsections': []}, {'subject_title': 'D HYPERPARAMETERS USED IN EXPERIMENTS', 'section_type': 'Section', 'starting_page_no': 18, 'ending_page_no': 18, 'entities': [{'model': 'RoBERTa'}, {'model': 'DeBERTa'}, {'publication year': '2019'}, {'publication year': '2021'}, {'author': 'Liu et al.'}, {'author': 'Houlsby et al.'}, {'author': 'Pfeiffer et al.'}, {'author': 'He et al.'}], 'subsections': [{'subject_title': 'D.1 ROBERTA', 'section_type': 'Subsection', 'starting_page_no': 18, 'ending_page_no': 18, 'entities': [], 'subsections': []}, {'subject_title': 'D.2 DEBERTA', 'section_type': 'Subsection', 'starting_page_no': 18, 'ending_page_no': 18, 'entities': [], 'subsections': []}]}, {'subject_title': 'Hyperparameters for RoBERTa on GLUE benchmark', 'section_type': 'Table', 'starting_page_no': 19, 'ending_page_no': 19, 'entities': [{'model': 'RoBERTa'}, {'benchmark': 'GLUE'}], 'subsections': []}, {'subject_title': 'GPT-2', 'section_type': 'Model Description', 'starting_page_no': 19, 'ending_page_no': 19, 'entities': [{'model': 'GPT-2'}, {'authors': 'Loshchilov & Hutter'}, {'year': '2017'}, {'authors': 'Li & Liang'}, {'year': '2021'}], 'subsections': []}, {'subject_title': 'GPT-3', 'section_type': 'Model Description', 'starting_page_no': 19, 'ending_page_no': 19, 'entities': [{'model': 'GPT-3'}, {'authors': 'Loshchilov & Hutter'}, {'year': '2017'}], 'subsections': []}, {'subject_title': 'Hyperparameters for DeBERTa XXL on GLUE benchmark', 'section_type': 'Table', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'dataset': 'MNLI'}, {'dataset': 'SST-2'}, {'dataset': 'MRPC'}, {'dataset': 'CoLA'}, {'dataset': 'QNLI'}, {'dataset': 'QQP'}, {'dataset': 'RTE'}, {'dataset': 'STS-B'}, {'model': 'DeBERTa XXL'}], 'subsections': []}, {'subject_title': 'Hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART', 'section_type': 'Table', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'dataset': 'E2E'}, {'dataset': 'WebNLG'}, {'dataset': 'DART'}, {'model': 'GPT-2 LoRA'}], 'subsections': []}, {'subject_title': 'Hyperparameter Tuning', 'section_type': 'Paragraph', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'paper': 'Zhong et al., 2017'}, {'paper': 'Williams et al., 2018'}, {'paper': 'Gliwa et al., 2019'}], 'subsections': []}, {'subject_title': 'Combining LoRA with Prefix Tuning', 'section_type': 'Section Header', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'model': 'LoRA'}, {'model': 'GPT-3'}], 'subsections': []}, {'subject_title': 'LoRA+PrefixEmbed (LoRA+PE)', 'section_type': 'Subsection', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'dataset': 'WikiSQL'}, {'dataset': 'MNLI'}], 'subsections': []}, {'subject_title': 'LoRA+PrefixLayer (LoRA+PL)', 'section_type': 'Subsection', 'starting_page_no': 20, 'ending_page_no': 20, 'entities': [{'dataset': 'WikiSQL'}, {'dataset': 'MNLI'}], 'subsections': []}, {'subject_title': 'Hyperparameters', 'section_type': 'Table Caption', 'starting_page_no': 21, 'ending_page_no': 21, 'entities': [{'optimizer': 'AdamW'}, {'batch size': 128}, {'epoch': 2}, {'warmup tokens': 250000}, {'learning rate': ['5.00E-06', '5.00E-04', '1.00E-04', '1.6E-03', '1.00E-04', '2.00E-04']}], 'subsections': []}, {'subject_title': 'F ADDITIONAL EMPIRICAL EXPERIMENTS', 'section_type': 'Section Header', 'starting_page_no': 21, 'ending_page_no': 21, 'entities': [{'dataset': 'WikiSQL'}, {'dataset': 'MultiNLI'}, {'dataset': 'DART'}, {'dataset': 'WebNLG'}, {'year': '2020'}, {'year': '2017'}, {'year': '2021'}], 'subsections': []}, {'subject_title': 'F.1 ADDITIONAL EXPERIMENTS ON GPT-2', 'section_type': 'Subsection Header', 'starting_page_no': 21, 'ending_page_no': 21, 'entities': [{'model': 'GPT-2 Medium'}, {'model': 'GPT-2 Large'}, {'method': 'Fine-Tune'}, {'method': 'AdapterL0'}, {'method': 'AdapterL1'}, {'method': 'FTTop2'}, {'method': 'PrefLayer'}, {'method': 'LoRA'}], 'subsections': []}, {'subject_title': 'Method WebNLG', 'section_type': 'Table', 'starting_page_no': 22, 'ending_page_no': 22, 'entities': [{'model': 'GPT-2 Medium'}, {'model': 'GPT-2 Large'}], 'subsections': []}, {'subject_title': 'F.2 ADDITIONAL EXPERIMENTS ON GPT-3', 'section_type': 'Section Header', 'starting_page_no': 22, 'ending_page_no': 22, 'entities': [{'model': 'GPT-3'}], 'subsections': []}, {'subject_title': 'F.3 LOW-DATA REGIME', 'section_type': 'Section Header', 'starting_page_no': 22, 'ending_page_no': 22, 'entities': [{'dataset': 'MNLI'}, {'dataset': 'MNLI-n'}], 'subsections': []}, {'subject_title': 'G MEASURING SIMILARITY BETWEEN SUBSPACES', 'section_type': 'Section Header', 'starting_page_no': 22, 'ending_page_no': 22, 'entities': [{'paper': 'Ham & Lee (2008)'}], 'subsections': []}, {'subject_title': 'Method Hyperparameters', 'section_type': 'Table', 'starting_page_no': 23, 'ending_page_no': 23, 'entities': [{'WikiSQL': 'dataset'}, {'MNLI-m': 'dataset'}, {'GPT-3': 'model'}, {'175B': 'model size'}], 'subsections': []}, {'subject_title': 'Hyperparameter analysis', 'section_type': 'Paragraph', 'starting_page_no': 23, 'ending_page_no': 23, 'entities': [{'LoRA': 'adaptation approach'}, {'PreﬁxEmbed': 'adaptation approach'}, {'PreﬁxLayer': 'adaptation approach'}], 'subsections': []}, {'subject_title': 'Validation accuracy of different methods', 'section_type': 'Table', 'starting_page_no': 23, 'ending_page_no': 23, 'entities': [{'GPT-3': 'model'}, {'MNLI': 'dataset'}, {'175B': 'model size'}], 'subsections': []}, {'subject_title': 'Projection Metric', 'section_type': 'Mathematical Definition', 'starting_page_no': 23, 'ending_page_no': 23, 'entities': [{'Ham & Lee': 'authors'}, {'2008': 'publication year'}], 'subsections': []}, {'subject_title': None, 'section_type': 'No Section Detected', 'starting_page_no': 24, 'ending_page_no': 24, 'entities': [], 'subsections': [], 'raw_text': 'Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\\nOptimizer - AdamW\\nWarmup Tokens - 250,000\\nLR Schedule - Linear\\nBatch Size - 20 20 100 128\\n# Epoch - 40 40 4 2\\nLearning RateFineTune 5.00E-6\\nPreﬁxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\\nPreﬁxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\\nLoRA 2.00E-4\\nPreﬁxEmbed lp 16 32 64 256\\nAdaptation- PreﬁxEmbed li 8\\nSpeciﬁc PreﬁxTune lp=li= 8\\nLoRA rq=rv= 8\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\\nwhere our similarity is deﬁned as:\\n\\x1e(A;B;i;j ) = (Ui\\nA;Uj\\nB) =Pp\\ni=1\\x1b2\\ni\\np=1\\np\\x10\\n1\\x00d(Ui\\nA;Uj\\nB)2\\x11\\nThis similarity satisﬁes that if Ui\\nAandUj\\nBshare the same column span, then \\x1e(A;B;i;j ) = 1 . If\\nthey are completely orthogonal, then \\x1e(A;B;i;j ) = 0 . Otherwise, \\x1e(A;B;i;j )2(0;1).\\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\nWe present additional results from our investigation into the low-rank update matrices.\\nH.1 C ORRELATION BETWEEN LORA M ODULES\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\\nlayers.\\nH.2 E FFECT OFrONGPT-2\\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\\ndataset as an example, we report the validation loss and test metrics achieved by different choices\\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\\nNote that the relationship between model size and the optimal rank for adaptation is still an open\\nquestion.\\nH.3 C ORRELATION BETWEEN WAND \\x01W\\nSee Figure 8 for the normalized subspace similarity between Wand\\x01Wwith varying r.\\nNote again that \\x01Wdoes not contain the top singular directions of W, since the similarity between\\nthe top 4 directions in \\x01Wand the top-10% of those in Wbarely exceeds 0.2. This gives evidence\\nthat\\x01Wcontains those “task-speciﬁc” directions that are otherwise notemphasized in W.\\nAn interesting next question to answer, is how “strong” do we need to amplify those task-speciﬁc\\ndirections, in order for the model adaptation to work well?\\n24'}, {'subject_title': 'Figure 6: Normalized subspace similarity', 'section_type': 'Figure', 'starting_page_no': 25, 'ending_page_no': 25, 'entities': [], 'subsections': []}, {'subject_title': 'H.4 Amplification Factor', 'section_type': 'Section', 'starting_page_no': 25, 'ending_page_no': 25, 'entities': [{'amplification factor': 20}, {'amplification factor': 2}, {'rank': 4}, {'rank': 64}], 'subsections': []}, {'subject_title': 'Figure 7', 'section_type': 'Image', 'starting_page_no': 26, 'ending_page_no': 26, 'entities': [{'layer': '96', 'model': 'Transformer'}], 'subsections': []}, {'subject_title': 'Table 18', 'section_type': 'Table', 'starting_page_no': 26, 'ending_page_no': 26, 'entities': [{'model': 'GPT-2 Medium', 'challenge': 'E2E NLG Challenge'}], 'subsections': []}, {'subject_title': 'Figure 8', 'section_type': 'Image', 'starting_page_no': 26, 'ending_page_no': 26, 'entities': [{'model': 'Wq', 'baseline': 'random'}], 'subsections': []}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data in JSON"
      ],
      "metadata": {
        "id": "a65BmJY6X_Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# structured_json is your Python dict/list/etc.\n",
        "json_str = json.dumps(structured_json,\n",
        "                      indent=4,          # pretty‐print with 4-space indents\n",
        "                      ensure_ascii=False # allow non-ASCII characters\n",
        "                     )\n",
        "print(json_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh1wrVKI3WpQ",
        "outputId": "bc8fdcde-4271-4f79-d0fb-eef431c47561"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"subject_title\": \"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
            "        \"section_type\": \"Document Title\",\n",
            "        \"starting_page_no\": 1,\n",
            "        \"ending_page_no\": 1,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"company name\": \"Microsoft Corporation\"\n",
            "            },\n",
            "            {\n",
            "                \"authors\": [\n",
            "                    \"Edward Hu\",\n",
            "                    \"Yelong Shen\",\n",
            "                    \"Phillip Wallis\",\n",
            "                    \"Zeyuan Allen-Zhu\",\n",
            "                    \"Yuanzhi Li\",\n",
            "                    \"Shean Wang\",\n",
            "                    \"Lu Wang\",\n",
            "                    \"Weizhu Chen\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"ABSTRACT\",\n",
            "        \"section_type\": \"Abstract\",\n",
            "        \"starting_page_no\": 1,\n",
            "        \"ending_page_no\": 1,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model name\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter count\": \"175B\"\n",
            "            },\n",
            "            {\n",
            "                \"company name\": \"Microsoft\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"INTRODUCTION\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 1,\n",
            "        \"ending_page_no\": 1,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model names\": [\n",
            "                    \"RoBERTa\",\n",
            "                    \"DeBERTa\",\n",
            "                    \"GPT-2\",\n",
            "                    \"GPT-3\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"authors\": [\n",
            "                    \"Radford et al.\",\n",
            "                    \"Liu et al.\",\n",
            "                    \"Brown et al.\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"publication years\": [\n",
            "                    \"b\",\n",
            "                    \"2019\",\n",
            "                    \"2020\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Introduction\",\n",
            "        \"section_type\": \"Abstract\",\n",
            "        \"starting_page_no\": 2,\n",
            "        \"ending_page_no\": 2,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"author\": \"Houlsby et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Rebufﬁ et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Li et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Aghajanyan et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2019\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2017\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2021\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2018\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Terminologies and Conventions\",\n",
            "        \"section_type\": \"Definition\",\n",
            "        \"starting_page_no\": 2,\n",
            "        \"ending_page_no\": 2,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"Transformer\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Vaswani et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Brown et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Loshchilov & Hutter\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Kingma & Ba\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"PROBLEM STATEMENT\",\n",
            "        \"section_type\": \"Problem Definition\",\n",
            "        \"starting_page_no\": 2,\n",
            "        \"ending_page_no\": 2,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Radford et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Brown et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Vaswani et al.\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Full Fine-Tuning\",\n",
            "        \"section_type\": \"Introduction\",\n",
            "        \"starting_page_no\": 3,\n",
            "        \"ending_page_no\": 3,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\",\n",
            "                \"parameter count\": \"175Billion\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Are Existing Solutions Good Enough?\",\n",
            "        \"section_type\": \"Related Work\",\n",
            "        \"starting_page_no\": 3,\n",
            "        \"ending_page_no\": 3,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2\",\n",
            "                \"authors\": [\n",
            "                    \"Houlsby et al.\",\n",
            "                    \"Rebufﬁ et al.\",\n",
            "                    \"Pfeiffer et al.\",\n",
            "                    \"R ¨uckl´e et al.\",\n",
            "                    \"Li & Liang\",\n",
            "                    \"Lester et al.\",\n",
            "                    \"Hambardzumyan et al.\",\n",
            "                    \"Liu et al.\",\n",
            "                    \"Lin et al.\",\n",
            "                    \"Ba et al.\",\n",
            "                    \"Radford et al.\",\n",
            "                    \"Shoeybi et al.\",\n",
            "                    \"Lepikhin et al.\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 4,\n",
            "        \"ending_page_no\": 4,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"Batch Size 32 16 1\\nSequence Length 512 256 128\\nj\\u0002j 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4\\u00060.8 338.0 \\u00060.6 19.8 \\u00062.7\\nAdapterL1482.0\\u00061.0 (+2.2%) 354.8 \\u00060.5 (+5.0%) 23.9 \\u00062.1 (+20.7%)\\nAdapterH1492.2\\u00061.0 (+3.0%) 366.3 \\u00060.5 (+8.4%) 25.8 \\u00062.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. “ j\\u0002j” denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\\nlearn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\\nweight matrix W02Rd\\u0002k, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ \\u0001W=W0+BA, whereB2Rd\\u0002r;A2Rr\\u0002k, and the rank r\\u001cmin(d;k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and\\u0001W=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiﬁed forward pass yields:\\nh=W0x+ \\u0001Wx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so\\u0001W=BAis zero at the beginning of training. We then scale \\u0001Wx by\\u000b\\nr, where\\u000b\\nis a constant in r. When optimizing with Adam, tuning \\u000bis roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set \\u000bto the ﬁrstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full ﬁne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preﬁx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\\u0002k.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B0A0, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"APPLYING LORA TO TRANSFORMER\",\n",
            "        \"section_type\": \"Methodology\",\n",
            "        \"starting_page_no\": 5,\n",
            "        \"ending_page_no\": 5,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"Transformer\"\n",
            "            },\n",
            "            {\n",
            "                \"module\": \"self-attention\"\n",
            "            },\n",
            "            {\n",
            "                \"module\": \"MLP\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"Wq\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"Wk\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"Wv\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"Wo\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"PRACTICAL BENEFITS AND LIMITATIONS\",\n",
            "        \"section_type\": \"Advantages and Limitations\",\n",
            "        \"starting_page_no\": 5,\n",
            "        \"ending_page_no\": 5,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3 175B\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"VRAM usage\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"checkpoint size\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"speedup\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"EMPIRICAL EXPERIMENTS\",\n",
            "        \"section_type\": \"Experiments\",\n",
            "        \"starting_page_no\": 5,\n",
            "        \"ending_page_no\": 5,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"RoBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"DeBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-3 175B\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"GLUE\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"SAMSum\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": [\n",
            "            {\n",
            "                \"subject_title\": \"BASELINES\",\n",
            "                \"section_type\": \"Sub-experiment\",\n",
            "                \"starting_page_no\": 5,\n",
            "                \"ending_page_no\": 5,\n",
            "                \"entities\": [\n",
            "                    {\n",
            "                        \"baseline\": \"Fine-Tuning (FT)\"\n",
            "                    },\n",
            "                    {\n",
            "                        \"baseline\": \"FTTop2\"\n",
            "                    }\n",
            "                ],\n",
            "                \"subsections\": []\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Model & Method\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 6,\n",
            "        \"ending_page_no\": 6,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"RoBERTa base\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"RoBERTa large\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"DeBERTa XXL\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"125.0M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"0.1M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"0.3M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"0.9M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"355.0M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"0.8M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"3.0M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"6.0M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"1500.0M\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter\": \"4.7M\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Description of Adaptation Methods\",\n",
            "        \"section_type\": \"Text\",\n",
            "        \"starting_page_no\": 6,\n",
            "        \"ending_page_no\": 6,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"author\": \"Houlsby et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Zaken et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Li & Liang\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Lin et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Pfeiffer et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Rückl´e et al.\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Model & Method\",\n",
            "        \"section_type\": \"Experiment Results\",\n",
            "        \"starting_page_no\": 7,\n",
            "        \"ending_page_no\": 7,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2 M\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2 L\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"BLEU\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"NIST\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"MET\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"ROUGE-L\"\n",
            "            },\n",
            "            {\n",
            "                \"metric\": \"CIDEr\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"5.2 ROBERTA BASE/LARGE\",\n",
            "        \"section_type\": \"Model Description\",\n",
            "        \"starting_page_no\": 7,\n",
            "        \"ending_page_no\": 7,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"RoBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Liu et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2019\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"BERT\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Devlin et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2019\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"5.3 DeBERTa AXXL\",\n",
            "        \"section_type\": \"Model Description\",\n",
            "        \"starting_page_no\": 7,\n",
            "        \"ending_page_no\": 7,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"DeBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"He et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2021\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"5.4 GPT-2 MEDIUM/LARGE\",\n",
            "        \"section_type\": \"Experiment Description\",\n",
            "        \"starting_page_no\": 7,\n",
            "        \"ending_page_no\": 7,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2 medium\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2 large\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Radford et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"b\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Model & Method\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 8,\n",
            "        \"ending_page_no\": 8,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MultiNLI-matched\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"SAMSum\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Scaling up to GPT-3 175B\",\n",
            "        \"section_type\": \"Text\",\n",
            "        \"starting_page_no\": 8,\n",
            "        \"ending_page_no\": 8,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter count\": \"175 billion\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Figure 2: GPT-3 175B validation accuracy\",\n",
            "        \"section_type\": \"Figure\",\n",
            "        \"starting_page_no\": 8,\n",
            "        \"ending_page_no\": 8,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MultiNLI-matched\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Related Works\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 8,\n",
            "        \"ending_page_no\": 8,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"Transformer\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Vaswani et al. (2017)\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Radford et al. (a)\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Devlin et al. (2019b)\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Radford et al. (b)\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Transformer Language Models\",\n",
            "        \"section_type\": \"Introduction\",\n",
            "        \"starting_page_no\": 9,\n",
            "        \"ending_page_no\": 9,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"company name\": \"None\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            },\n",
            "            {\n",
            "                \"model name\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter count\": \"175B\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Prompt Engineering and Fine-Tuning\",\n",
            "        \"section_type\": \"Subsection\",\n",
            "        \"starting_page_no\": 9,\n",
            "        \"ending_page_no\": 9,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model name\": \"GPT-3 175B\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Parameter-Efficient Adaptation\",\n",
            "        \"section_type\": \"Subsection\",\n",
            "        \"starting_page_no\": 9,\n",
            "        \"ending_page_no\": 9,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model name\": \"COMPACTER\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2021\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Low-Rank Structures in Deep Learning\",\n",
            "        \"section_type\": \"Subsection\",\n",
            "        \"starting_page_no\": 9,\n",
            "        \"ending_page_no\": 9,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"publication year\": \"2016\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2010\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2018\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2013\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"UNDERSTANDING THE LOW-RANK UPDATES\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 9,\n",
            "        \"ending_page_no\": 9,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model name\": \"LoRA\"\n",
            "            },\n",
            "            {\n",
            "                \"model name\": \"GPT-3 175B\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 10,\n",
            "        \"ending_page_no\": 10,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3 175B\"\n",
            "            },\n",
            "            {\n",
            "                \"parameter budget\": \"18M\"\n",
            "            },\n",
            "            {\n",
            "                \"storage size\": \"35MB\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"7.2 WHAT IS THE OPTIMAL RANKrFOR LORA?\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 10,\n",
            "        \"ending_page_no\": 10,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 11,\n",
            "        \"ending_page_no\": 11,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\\u0014i\\u00148) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1\\u0014j\\u001464)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\n\\u001e(Ar=8;Ar=64;i;j) =jjUi>\\nAr=8Uj\\nAr=64jj2\\nF\\nmin(i;j)2[0;1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\n\\u001e(\\u0001)has a range of [0;1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how \\u001echanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both \\u0001Wqand\\u0001Wv.\\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiﬁcantly between\\nAr=8andAr=64, while others do not. Speciﬁcally, \\u0001Wv(resp. \\u0001Wq) ofAr=8\\nand\\u0001Wv(resp. \\u0001Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0:5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conﬁrm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n\\u0001Wqappears to have a higher “intrinsic rank” than \\u0001Wv, since more common singular value direc-\\ntions are learned by both runs for \\u0001Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX \\u0001WCOMPARE TO W?\\nWe further investigate the relationship between \\u0001WandW. In particular, does \\u0001Whighly correlate\\nwithW? (Or mathematically, is \\u0001Wmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices – we stick with\\nAfor our experiments.\\n11\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Figure 4\",\n",
            "        \"section_type\": \"Figure\",\n",
            "        \"starting_page_no\": 12,\n",
            "        \"ending_page_no\": 12,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"layer\": \"48-th layer\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Table 7\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 12,\n",
            "        \"ending_page_no\": 12,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"layer\": \"48th layer\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"CONCLUSION AND FUTURE WORK\",\n",
            "        \"section_type\": \"Conclusion\",\n",
            "        \"starting_page_no\": 12,\n",
            "        \"ending_page_no\": 12,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"Transformer language models\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 13,\n",
            "        \"ending_page_no\": 13,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deﬁciency of \\u0001Wsuggests that Wcould\\nbe rank-deﬁcient as well, which can also be a source of inspiration for future works.\\nREFERENCES\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\\nhttp://arxiv.org/abs/2012.13255 .\\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Kernels? In\\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\\n03962 .\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\\nmatrix completion. SIAM Journal on optimization , 20(4):1956–1982, 2010.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep\\nneural networks with multitask learning. In Proceedings of the 25th international conference\\non Machine learning , ICML ’08, pp. 160–167, New York, NY , USA, July 2008. Association\\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\\nhttps://doi.org/10.1145/1390156.1390177 .\\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\\nparameters in deep learning, 2014.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019a.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\\nhttps://aclanthology.org/I05-5002 .\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\\nNatural Language Generation , pp. 124–133, 2017.\\n13\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 14,\n",
            "        \"ending_page_no\": 14,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\\nhttp://arxiv.org/abs/1911.12237 .\\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\\napproximation techniques. GAMM-Mitteilungen , 36(1):53–78, 2013.\\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\\nlearning. In ICML , pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.\\n1390204 .\\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\\n2101.00121 . arXiv: 2101.00121.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning\\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\\n00751 .\\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\\nof factorized neural layers, 2021.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt\\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\\narXiv: 2104.08691.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\\nXiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation.\\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\\n2358–2367. PMLR, 2016.\\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\\nory, pp. 2–47. PMLR, 2018b.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020 , pp. 441–459, Online, November 2020. Association for Computational\\nLinguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.41. URL https://aclanthology.\\norg/2020.findings-emnlp.41 .\\n14\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 15,\n",
            "        \"ending_page_no\": 15,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\\n2103.10385 . arXiv: 2103.10385.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank\\nhypercomplex adapter layers, 2021.\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.\\nJekaterina Novikova, Ond ˇrej Du ˇsek, and Verena Rieser. The e2e dataset: New challenges for end-\\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\\narXiv:1906.05392 , 2019.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\\nInterspeech , pp. 3743–3747, 2018.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\\nstanding by Generative Pre-Training. pp. 12, a.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nModels are Unsupervised Multitask Learners. pp. 24, b.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\\nabs/1705.08045 . arXiv: 1705.08045.\\nAndreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\\nIryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers, 2020.\\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\\nrank matrix factorization for deep neural network training with high-dimensional output targets.\\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655–\\n6659. IEEE, 2013.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\\nallelism, 2020.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\\nProcessing , pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\\n15\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"References\",\n",
            "        \"section_type\": \"References\",\n",
            "        \"starting_page_no\": 16,\n",
            "        \"ending_page_no\": 16,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"author\": \"Ashish Vaswani\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Alex Wang\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Alex Warstadt\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Adina Williams\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Thomas Wolf\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Greg Yang\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Elad Ben Zaken\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Yu Zhang\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Yong Zhao\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Victor Zhong\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2017\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2018\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2019\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2021\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 16,\n",
            "        \"ending_page_no\": 16,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Brown et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Method\",\n",
            "        \"section_type\": \"Methodology\",\n",
            "        \"starting_page_no\": 17,\n",
            "        \"ending_page_no\": 17,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"company name\": \"None\",\n",
            "                \"publication year\": \"2020\",\n",
            "                \"model name\": \"GPT-2 medium\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"B I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\",\n",
            "        \"section_type\": \"Discussion\",\n",
            "        \"starting_page_no\": 17,\n",
            "        \"ending_page_no\": 17,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"author name\": \"Lin\",\n",
            "                \"publication year\": \"2019\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"C D ATASET DETAILS\",\n",
            "        \"section_type\": \"Dataset Description\",\n",
            "        \"starting_page_no\": 17,\n",
            "        \"ending_page_no\": 17,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset name\": \"RTE\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Datasets\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 18,\n",
            "        \"ending_page_no\": 18,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"GLUE\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"SAMSum\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"E2E NLG Challenge\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"DART\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WebNLG\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2017\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2019\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2020\"\n",
            "            },\n",
            "            {\n",
            "                \"license\": \"BSD 3-Clause License\"\n",
            "            },\n",
            "            {\n",
            "                \"license\": \"Creative Commons BY-NC-ND 4.0\"\n",
            "            },\n",
            "            {\n",
            "                \"license\": \"Creative Commons BY-NC-SA 4.0\"\n",
            "            },\n",
            "            {\n",
            "                \"license\": \"MIT license\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"D HYPERPARAMETERS USED IN EXPERIMENTS\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 18,\n",
            "        \"ending_page_no\": 18,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"RoBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"DeBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2019\"\n",
            "            },\n",
            "            {\n",
            "                \"publication year\": \"2021\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Liu et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Houlsby et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"Pfeiffer et al.\"\n",
            "            },\n",
            "            {\n",
            "                \"author\": \"He et al.\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": [\n",
            "            {\n",
            "                \"subject_title\": \"D.1 ROBERTA\",\n",
            "                \"section_type\": \"Subsection\",\n",
            "                \"starting_page_no\": 18,\n",
            "                \"ending_page_no\": 18,\n",
            "                \"entities\": [],\n",
            "                \"subsections\": []\n",
            "            },\n",
            "            {\n",
            "                \"subject_title\": \"D.2 DEBERTA\",\n",
            "                \"section_type\": \"Subsection\",\n",
            "                \"starting_page_no\": 18,\n",
            "                \"ending_page_no\": 18,\n",
            "                \"entities\": [],\n",
            "                \"subsections\": []\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameters for RoBERTa on GLUE benchmark\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 19,\n",
            "        \"ending_page_no\": 19,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"RoBERTa\"\n",
            "            },\n",
            "            {\n",
            "                \"benchmark\": \"GLUE\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"GPT-2\",\n",
            "        \"section_type\": \"Model Description\",\n",
            "        \"starting_page_no\": 19,\n",
            "        \"ending_page_no\": 19,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2\"\n",
            "            },\n",
            "            {\n",
            "                \"authors\": \"Loshchilov & Hutter\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2017\"\n",
            "            },\n",
            "            {\n",
            "                \"authors\": \"Li & Liang\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2021\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"GPT-3\",\n",
            "        \"section_type\": \"Model Description\",\n",
            "        \"starting_page_no\": 19,\n",
            "        \"ending_page_no\": 19,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            },\n",
            "            {\n",
            "                \"authors\": \"Loshchilov & Hutter\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2017\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameters for DeBERTa XXL on GLUE benchmark\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"MNLI\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"SST-2\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MRPC\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"CoLA\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"QNLI\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"QQP\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"RTE\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"STS-B\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"DeBERTa XXL\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"E2E\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WebNLG\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"DART\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2 LoRA\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameter Tuning\",\n",
            "        \"section_type\": \"Paragraph\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"paper\": \"Zhong et al., 2017\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Williams et al., 2018\"\n",
            "            },\n",
            "            {\n",
            "                \"paper\": \"Gliwa et al., 2019\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Combining LoRA with Prefix Tuning\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"LoRA\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"LoRA+PrefixEmbed (LoRA+PE)\",\n",
            "        \"section_type\": \"Subsection\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MNLI\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"LoRA+PrefixLayer (LoRA+PL)\",\n",
            "        \"section_type\": \"Subsection\",\n",
            "        \"starting_page_no\": 20,\n",
            "        \"ending_page_no\": 20,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MNLI\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameters\",\n",
            "        \"section_type\": \"Table Caption\",\n",
            "        \"starting_page_no\": 21,\n",
            "        \"ending_page_no\": 21,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"optimizer\": \"AdamW\"\n",
            "            },\n",
            "            {\n",
            "                \"batch size\": 128\n",
            "            },\n",
            "            {\n",
            "                \"epoch\": 2\n",
            "            },\n",
            "            {\n",
            "                \"warmup tokens\": 250000\n",
            "            },\n",
            "            {\n",
            "                \"learning rate\": [\n",
            "                    \"5.00E-06\",\n",
            "                    \"5.00E-04\",\n",
            "                    \"1.00E-04\",\n",
            "                    \"1.6E-03\",\n",
            "                    \"1.00E-04\",\n",
            "                    \"2.00E-04\"\n",
            "                ]\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"F ADDITIONAL EMPIRICAL EXPERIMENTS\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 21,\n",
            "        \"ending_page_no\": 21,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"WikiSQL\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MultiNLI\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"DART\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"WebNLG\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2020\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2017\"\n",
            "            },\n",
            "            {\n",
            "                \"year\": \"2021\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"F.1 ADDITIONAL EXPERIMENTS ON GPT-2\",\n",
            "        \"section_type\": \"Subsection Header\",\n",
            "        \"starting_page_no\": 21,\n",
            "        \"ending_page_no\": 21,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2 Medium\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2 Large\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"Fine-Tune\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"AdapterL0\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"AdapterL1\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"FTTop2\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"PrefLayer\"\n",
            "            },\n",
            "            {\n",
            "                \"method\": \"LoRA\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Method WebNLG\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 22,\n",
            "        \"ending_page_no\": 22,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2 Medium\"\n",
            "            },\n",
            "            {\n",
            "                \"model\": \"GPT-2 Large\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"F.2 ADDITIONAL EXPERIMENTS ON GPT-3\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 22,\n",
            "        \"ending_page_no\": 22,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-3\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"F.3 LOW-DATA REGIME\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 22,\n",
            "        \"ending_page_no\": 22,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"dataset\": \"MNLI\"\n",
            "            },\n",
            "            {\n",
            "                \"dataset\": \"MNLI-n\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"G MEASURING SIMILARITY BETWEEN SUBSPACES\",\n",
            "        \"section_type\": \"Section Header\",\n",
            "        \"starting_page_no\": 22,\n",
            "        \"ending_page_no\": 22,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"paper\": \"Ham & Lee (2008)\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Method Hyperparameters\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 23,\n",
            "        \"ending_page_no\": 23,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"WikiSQL\": \"dataset\"\n",
            "            },\n",
            "            {\n",
            "                \"MNLI-m\": \"dataset\"\n",
            "            },\n",
            "            {\n",
            "                \"GPT-3\": \"model\"\n",
            "            },\n",
            "            {\n",
            "                \"175B\": \"model size\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Hyperparameter analysis\",\n",
            "        \"section_type\": \"Paragraph\",\n",
            "        \"starting_page_no\": 23,\n",
            "        \"ending_page_no\": 23,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"LoRA\": \"adaptation approach\"\n",
            "            },\n",
            "            {\n",
            "                \"PreﬁxEmbed\": \"adaptation approach\"\n",
            "            },\n",
            "            {\n",
            "                \"PreﬁxLayer\": \"adaptation approach\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Validation accuracy of different methods\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 23,\n",
            "        \"ending_page_no\": 23,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"GPT-3\": \"model\"\n",
            "            },\n",
            "            {\n",
            "                \"MNLI\": \"dataset\"\n",
            "            },\n",
            "            {\n",
            "                \"175B\": \"model size\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Projection Metric\",\n",
            "        \"section_type\": \"Mathematical Definition\",\n",
            "        \"starting_page_no\": 23,\n",
            "        \"ending_page_no\": 23,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"Ham & Lee\": \"authors\"\n",
            "            },\n",
            "            {\n",
            "                \"2008\": \"publication year\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": null,\n",
            "        \"section_type\": \"No Section Detected\",\n",
            "        \"starting_page_no\": 24,\n",
            "        \"ending_page_no\": 24,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": [],\n",
            "        \"raw_text\": \"Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\\nOptimizer - AdamW\\nWarmup Tokens - 250,000\\nLR Schedule - Linear\\nBatch Size - 20 20 100 128\\n# Epoch - 40 40 4 2\\nLearning RateFineTune 5.00E-6\\nPreﬁxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\\nPreﬁxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\\nLoRA 2.00E-4\\nPreﬁxEmbed lp 16 32 64 256\\nAdaptation- PreﬁxEmbed li 8\\nSpeciﬁc PreﬁxTune lp=li= 8\\nLoRA rq=rv= 8\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\\nwhere our similarity is deﬁned as:\\n\\u001e(A;B;i;j ) = (Ui\\nA;Uj\\nB) =Pp\\ni=1\\u001b2\\ni\\np=1\\np\\u0010\\n1\\u0000d(Ui\\nA;Uj\\nB)2\\u0011\\nThis similarity satisﬁes that if Ui\\nAandUj\\nBshare the same column span, then \\u001e(A;B;i;j ) = 1 . If\\nthey are completely orthogonal, then \\u001e(A;B;i;j ) = 0 . Otherwise, \\u001e(A;B;i;j )2(0;1).\\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\nWe present additional results from our investigation into the low-rank update matrices.\\nH.1 C ORRELATION BETWEEN LORA M ODULES\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\\nlayers.\\nH.2 E FFECT OFrONGPT-2\\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\\ndataset as an example, we report the validation loss and test metrics achieved by different choices\\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\\nNote that the relationship between model size and the optimal rank for adaptation is still an open\\nquestion.\\nH.3 C ORRELATION BETWEEN WAND \\u0001W\\nSee Figure 8 for the normalized subspace similarity between Wand\\u0001Wwith varying r.\\nNote again that \\u0001Wdoes not contain the top singular directions of W, since the similarity between\\nthe top 4 directions in \\u0001Wand the top-10% of those in Wbarely exceeds 0.2. This gives evidence\\nthat\\u0001Wcontains those “task-speciﬁc” directions that are otherwise notemphasized in W.\\nAn interesting next question to answer, is how “strong” do we need to amplify those task-speciﬁc\\ndirections, in order for the model adaptation to work well?\\n24\"\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Figure 6: Normalized subspace similarity\",\n",
            "        \"section_type\": \"Figure\",\n",
            "        \"starting_page_no\": 25,\n",
            "        \"ending_page_no\": 25,\n",
            "        \"entities\": [],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"H.4 Amplification Factor\",\n",
            "        \"section_type\": \"Section\",\n",
            "        \"starting_page_no\": 25,\n",
            "        \"ending_page_no\": 25,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"amplification factor\": 20\n",
            "            },\n",
            "            {\n",
            "                \"amplification factor\": 2\n",
            "            },\n",
            "            {\n",
            "                \"rank\": 4\n",
            "            },\n",
            "            {\n",
            "                \"rank\": 64\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Figure 7\",\n",
            "        \"section_type\": \"Image\",\n",
            "        \"starting_page_no\": 26,\n",
            "        \"ending_page_no\": 26,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"layer\": \"96\",\n",
            "                \"model\": \"Transformer\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Table 18\",\n",
            "        \"section_type\": \"Table\",\n",
            "        \"starting_page_no\": 26,\n",
            "        \"ending_page_no\": 26,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"GPT-2 Medium\",\n",
            "                \"challenge\": \"E2E NLG Challenge\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    },\n",
            "    {\n",
            "        \"subject_title\": \"Figure 8\",\n",
            "        \"section_type\": \"Image\",\n",
            "        \"starting_page_no\": 26,\n",
            "        \"ending_page_no\": 26,\n",
            "        \"entities\": [\n",
            "            {\n",
            "                \"model\": \"Wq\",\n",
            "                \"baseline\": \"random\"\n",
            "            }\n",
            "        ],\n",
            "        \"subsections\": []\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save JSON Output"
      ],
      "metadata": {
        "id": "sEX63RX228mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Save JSON Output\n",
        "output_filename = \"parsed_document.json\"\n",
        "with open(output_filename, \"w\") as f:\n",
        "    json.dump(structured_json, f, indent=2)\n",
        "\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "E6KVavdc2578",
        "outputId": "b892e662-3573-436b-87bd-854f8d29f03d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ccd50a1e-1b85-4f4a-a707-d74f13608110\", \"parsed_document.json\", 47394)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}